{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "import timm\n",
    "import torch\n",
    "\n",
    "from utils.torch2onnx import convert_to_onnx_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CHANNELS = 3\n",
    "INPUT_HEIGHT = 1024\n",
    "INPUT_WIDTH = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cpu = timm.create_model(model_name='resnet18')\n",
    "model_cpu.eval()\n",
    "\n",
    "model_gpu = timm.create_model(model_name='resnet18')\n",
    "model_gpu.to('cuda:0')\n",
    "model_gpu.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnxmodel, _ = convert_to_onnx_model(\n",
    "                    model_cpu,\n",
    "                    input_shape=(1, N_CHANNELS, INPUT_HEIGHT, INPUT_WIDTH),\n",
    "                    output_names=['output'],\n",
    "                    dynamic_axes={'input': {0: 'batch_size', 2: 'height', 3: 'width'}}\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_array = np.random.randn(1, N_CHANNELS, INPUT_HEIGHT, INPUT_WIDTH).astype(np.float32)\n",
    "input_tensor = torch.from_numpy(input_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Inference with CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "with torch.inference_mode():\n",
    "    result_torch = model_cpu(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Infenrence with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "with torch.inference_mode():\n",
    "    _input_tensor = torch.from_numpy(input_array).to('cuda:0')\n",
    "    result_torch = model_gpu(_input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX Inference with CPUExecutionProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_cpu = onnxruntime.InferenceSession(onnxmodel.SerializeToString(),\n",
    "                                           providers=['CPUExecutionProvider']\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "result_onnx = session_cpu.run(None, {'input': input_array})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX Inference with CudaExecutionProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_gpu = onnxruntime.InferenceSession(onnxmodel.SerializeToString(),\n",
    "                                           providers=['CudaExecutionProvider']\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "result_onnx = session_gpu.run(None, {'input': input_array})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenVINO Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino.runtime as ov\n",
    "\n",
    "core = ov.Core()\n",
    "ovmodel = core.read_model(onnxmodel.SerializeToString())\n",
    "compiled_model = core.compile_model(model=ovmodel, device_name='CPU')\n",
    "infer_request = compiled_model.create_infer_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "result_ov = infer_request.infer(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenVINO Async Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_requests = []\n",
    "for i in range(4):\n",
    "    infer_requests.append(compiled_model.create_infer_request())\n",
    "\n",
    "\n",
    "for ir in infer_requests:\n",
    "        ir.start_async(input_array, share_inputs=True)\n",
    "        \n",
    "        \n",
    "preds = []\n",
    "for ir in infer_requests:\n",
    "    ir.wait()\n",
    "    preds.append(ir.results['output'])\n",
    "\n",
    "preds = np.stack(preds).mean(axis=0)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
